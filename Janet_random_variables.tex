\section{Binomial and Hypergeometric}\smallskip \hrule height 2pt \smallskip
Note: this is for objects that are indistinguishable.  You can distinguish a red fish from a blue fish but you can't tell whether a red fish's name is "Mary" or "Gary". 
\hfill \\

\subsection{Binomial distribution/"probability"}
	\begin{description}
		
		\item[Binomial Distribution] ("Probability" in 10/12 lecture). A series of \textbf{independent trials}, each resulting in one of \textbf{two} possible outcomes, "success", or "failure".  So yes replacement if it keeps trials independent, and can't have three possible outcomes. 
			\[ P(k \mbox \ successes) = {{n}\choose{k}}p^k(1-p)^k \mbox{, for \ } k = 0, 1, \dots, n \]
		Note this is the number of ways of getting $k$ indistinguishable objects in n tries.  Then you multiply by the probability, $p^k$, of getting that number $k$, and the probability $(1-p)^k$ of getting the rest.  $P(\mbox{2 heads out of three for a coin that gives heads 3/4 of the time}) = (3)*(0.75^2)*(0.25^1)$ \hfill \\
		\hfill \\
	\end{description}

\subsection{Hypergeometric Distribution}
	\begin{description}
		\item[Hypergeometric Distribution]  For \textbf{no replacement}.  Draw $n$ chips from an urn that has $r$ red chips, $w$ white chips, and $r + w = N$.   $n$ is the total number of chips you will draw; $k/n$ will be the fraction you draw that are red. Draw chips without replacement.  Note k must be $<$ r or the problem doesn't make sense and something like ${{r} \choose {k}} = {{1} \choose {5}} = ?!?!$ can happen.  
			\[ P(\mbox{k red chips chosen}) = \frac{ {{r} \choose {k}} {{w} \choose {n-k}} }{{{N}\choose{k}}} \] % \frac{{{r}\choose{k}}{w \chose {n-k}}}{N \chose k} \]
			\[ \frac{\mbox{(ways to draw \ k red from r)}*\mbox{(ways to draw n-k non-red from r)}}{\mbox{total number of combinations of chips you can draw}} \] \hfill \\
		Note: this is the same as	{\tiny pg 111}
		\[ = \frac{ {{n} \choose {k}} ({{_r}P_{k}}) ({{_w}P_{n-k}}) }{{{_N}P_n}} \] 
			\hfill \\
		Note that if you sample a very small fraction of the fish pond you will recover the binomial distribution (because replacement vs. no replacement isn't so important). \hfill \\
		\hfill \\
		Say you have $1, 2, 3, \dots, t$ types of objects with numbers $n_1, n_2, \dots, n_t$ in an urn.
		If you want to chose $k_1$ objects of  type 1, $k_2$ objects of  type 2, $\dots$ $k_t$ objects of  type $t$.\
		Then the number of objects is $N = n_1 + n_2 + \dots + n_t$, the number you are selecting is $n = k_1+ k_2 + \dots + k_t$, and  the formula is   {\tiny q 3.2.35, pg 118}
		\[  \frac{ {{n_1}\choose{k_1}} {{n_2}\choose{k_2}} \dots {{n_t}\choose{k_t}} }{ {{N}\choose{n}} }  \]
		This is different than the formula I derived.  I was multiplying the probabilities separately, and updating N each time.  Is this an overestimation? 		
	\end{description}
	
\section{Random Variables} 

\begin{center}
\begin{tabular}{ c c c c c }
 \textbf{R.Var.} & \textbf{pdf} & \textbf{cdf} &   \\ 
 	discrete &   pmf (m $=$ mass) & cdf \\
	continuous & pdf (d $=$ density) & cdf
 \end{tabular}
\end{center}
\hfill \\
A probability mass function (pmf) is a function that gives the probability that a discrete random variable is exactly equal to some value.
A probability mass function differs from a probability density function (pdf) in that the latter is associated with continuous rather than discrete random variables; the values of the latter are not probabilities as such: a pdf must be integrated over an interval to yield a probability.
%\begin{center}
%\begin{tabular}{ c c c c c }
 %\textbf{R.Var.} & \textbf{density} & \textbf{cumulative} &   \\ 
 %	discrete & pmf  or pdf & cdf \\
%	continuous & pdf & cdf
 %\end{tabular}
%\end{center}
%\hfill \\


\subsection{Discrete probability function}
Suppose S is a finite or countably infinite sample space.  Let $p$ be a real-valued function defined for each element of $S$ such that: {\tiny pg 119} \hfill \\
(a)  \[ 0 < p(s) \mbox{ for each \ } s \in S  \] 
(b)  \[  \sum\limits_{s \in S} p(s) = 1 \]
Then $p$ is said to be a \textit{discrete probability function} \hfill \\
\hfill \\
Once $p(s)$ is defined for all $s$, then you can say the probability of any event $A$ is the sum of the probabilities of the outcomes comprising $A$: 
\[  P(A) = \sum\limits_{s \in S} p(s) \]
This function satisfies the probability axioms of Section 2.3.   \hfill \\

Note: you can still have an infinite number of outcomes in the sample space, as long as the probability of all outcomes sums to one.  E.g. probability of getting heads on an odd numbered coin toss has an infinite number of events but if you sum the two sums (got it on odd, got it on even), you get a sum of 1.  {\tiny pg 120}

\subsection{Discrete random variable}
3 Lecture Axioms: {\tiny 10/14}:   $\Omega \rightarrow {\cal X} \subseteq $ R   \hfill \\  
(The sample space is ${\cal X}$, which is real.)  Pg 119 has a similar set of facts, that are presented a little differently. 
 \hfill \\  
	\[  P(X=x) \geq 0 \mbox{, \ \  }  P(X=x) > 0  \mbox{ if } x \in {\cal X}  \]
%(book version (pg 119)): $ 0 \leq p(s)$ for each $s \in S$. ($S$ is a finite or countably infinite sample space. P is a real -valued function defined over eac element of S) 
	\[  P(\Omega) = 1 \mbox{, so \ }  \sum\limits_{x \in \cal X} P(x= {\cal X}) = 1\]
$E_1$, $E_2$, $\dots$, $E_k$ are disjoint (non-overlapping). 
	\[ P(E_1 \cup E_2 \cup \dots \cup E_k) =  \sum\limits_{i=1} P(E_i) \mbox{;} P(x \in B) = \sum\limits_{{\cal X} \in B} P(x = {\cal X}) \]
We have done this.  E.g. P(X is 2, 3, or 4) $= P(X=2)$ + $P(X=3)$ + $P(X=4)$
	%\[  E_1, E_2, \dots, E_k \mbox{ are disjoint (non-ovaerlapping).} P(E_1 \cup E_2 \cup \dots \cup E_k) =  \sum\limits_{i=1} P(E_i) \]
	%\[  P(\Omega) = 1, so  \sum\limits_{i=1}^k P(E_i)  \mbox{ if } x \in {\cal X} \]
	
     \hfill \\  
  \hfill \\  
A function whose domain is a sample space $S$ and whose values form a finite or countably infinite set of real numbers is called a \textbf{discrete random variable}.  We denote random variables by uppercase letters, often X or Y.  {\tiny page 123}  \hfill \\
Example: sum of the values of two die faces.  E.g. value of die 1 is $X_1$, value of die 2 is $X_2$, value of sum is $X = X_1 + X_2$    \hfill \\  
  \hfill \\  
  
\textbf{The Probability Density Function}.  Associated with every discrete random variable X is a \textit{probability density function} (or \textit{pdf}), denoted $p_x(k)$ where 
	\[  p_X(k) = P({s \in S \mid X(s) = k})  \]
That's often written as $ p_X(k) = P(X = k) $.  \hfill \\  
Note: the binomial distribution is such an example:   (So is hypergeometric.) 
	\[  p_x(k) = P(k \mbox \ successes) = {{n}\choose{k}}p^k(1-p)^k \mbox{, for \ } k = 0, 1, \dots, n \]

\subsection{Cumulative Distribution Function} \textbf{(Discrete)}
You might want $P(x \leq X \leq t) = P(X \leq t) - P(X \leq s - 1)$  {\tiny pg 127}  \hfill \\  
\textbf{Cumulative distribution function}:  Let $X$ be a discrete random variable.  For any real number $t$, the probability that $X$ takes on a value $\leq t$ is the \textit{cumulative distribution} (cdf) of $X$ [written $F_X(t)$].
	\[ F_X(t) = P({s \ in S \mid X(s) \leq t}) \]
or more simply 
	\[  F_X(t) = P(X \leq t)  \]
then {\tiny 10/19 lecture}
	\[  P(a \leq X \leq b) = F_X(b) - F_X(a)  \]
	
\subsection{Continuous random variables}
A probability function $P$ on a set of real numbers $S$ is called \textbf{continuous} if there exists a function $f(t)$ such that for any closed interval $[a, b] \subset S$, $P([a, b]) = \int_{a}^{b} f(t)dt$. \hfill \\  
Kind of obvious, but all values of the function must be more than zero.  \hfill \\  

\subsection{Continuous probability density functions}
Density functions are \textbf{not a probabiliy}.  {\tiny (11/4 lecture)}  \hfill \\ 
\hfill \\  

{\tiny (pg 135)} Let $Y$ be a function from a sample space $S$ to the real-numbers (takes values of real numbers; put in something and you get out a real number).  The function $Y$ is called a \textit{continuous random variable} if there exists a function $f_Y(y)$ such that for any real numbers $a$ and $b$ with $a < b$:
	\[  P(a \leq Y \leq b) = \int_{a}^{b} f_Y(y)dy \]
The function $f_Y(y)$ is the \textbf{\textit{probability density function (pdf)}} for $Y$.  Think of "density" as corresponding to how much height there is over the x-axis when you plot $f_Y(y)$ versus $y$.  \hfill \\

\hfill \\ 
\textbf{The value of $f$ can be greater than 1}.  {\tiny See 10/28 lecture.}   If $f_X(x) = c$ for $ 0 \leq x \leq 1/2$ then the area has to $= 1$ so $c = 2$.  So $p_X(x) = 2$. 

\subsection{Continuous cumulative distribution functions}
As in the discrete case, the \textit{cumulative distribution function (cdf)} is defined by $F_Y(y) = P(Y \leq y)$: {\tiny (pg 136)}
	\[  F_Y(y) = P(Y \leq y) = \int_{- \inf}^{y} f_Y(t)dt \]
Also written as  {\tiny (Definition 3.4.3: pg 137)}
	\[  F_Y(y) = \int_{- \inf}^{y} f_Y(r)dr = P({s \in S \mid Y(s) \leq y}) = P(Y < y) \]
The cdf in this case is an integral of $f_Y(y)$.  Note that now we have $f_Y(t)$ or $f_Y(r)$, not $f_Y(y)$.  We are still integrating over the x-axis ($y$) but we can't have $y$ in $dy$ and in the integration limits.   \hfill \\  
Also note the derivative of the cdf is the pdf:
	\[  \frac{d}{dy}F_Y(y) = f_Y(y) \]
 \hfill \\  
  \hfill \\  
\subsection{Indepenence}	
If X, Y are independent, $P_{X,Y}(x,y) = P_X(x) \cdot P_Y(y)$ for all $x$, $y$.
Note there are cases when $E(XY) = E(X)E(Y)$ but this does \textbf{not} imply independence. {\tiny (See Lec 11/4)}
 \hfill \\  	
 \hfill \\  
 
\subsection{Expected Values}
Let $X$ be a discrete random variable with probability function $p_X(k)$.  The \textit{expected value} of $X$ is denoted $E(X)$ (or sometimes $\mu$ or $\mu x$ and is given by: {\tiny pg 140}   \hfill \\  
\textbf{Discrete:}
	\[  E(X) = \mu = \mu x = \sum\limits_{\mbox{all } k} k*p_X(k)  \]
\textbf{Random:}	  \hfill \\  
	\[  E(Y) = \mu = \mu y = \int_{- \inf}^{\inf} y*f(y)dy  \]
\textbf{binomial:} {\tiny pg 141, ~10/16 TA lecture} 	  \hfill \\
	\[ E(X) = np =  \sum\limits_{k=0}^{n} k \cdot p_X(k) = \sum\limits_{k=0}^{n}k \cdot {{n}\choose{k}}p^k(1-p)^k \]  
\textbf{hypergeometric}: for selecting $n$ balls from $r$ red balls and $w$ white balls{\tiny pg 143, ~10/16 TA lecture}  \hfill \\  
	\[ E(X) = \frac{rn}{r+w} \]
turns out the same if you substitute the proportion of red balls:
	\[ E(X) = n \cdot p \mbox{ for } p= \frac{r}{r+w} \]
\hfill \\  	

\subsection{The median}
If $X$ is a discrete random variable, the median, $m$, is that point for which $P(X < m) = P(X>m)$.  In the event that $P(X \leq m) = 0.5$ and $P(X \geq m')$, the median is defined to be the arithmetic average, $(m + m')/2$.  \hfill \\
If $Y$ is a continuous random variable, its median is the solution to the integral equation $\int_{- \inf}^{m} f(y)dy = 0.5$  \hfill \\
If a random variable's pdf is symmetric, both $\mu$ and $m$ will be equal.  

\subsection{Variance}
{\tiny (pg 157; Sect3.6)}  
The variance of a random variable is the expected value of its squared deviations from $\mu$.  
Visually, it is the moment of inertia. \hfill \\  
Always positive. {\tiny -E.T. MT2 review} \hfill \\  
\hfill \\  
Discrete: $Var(X) = \sigma ^2 = E[(X-\mu)^2] = \sum \limits_{\mbox{all } k} (k - \mu)^2 \cdot f_X(k)$  \hfill \\  
\hfill \\  
Continuous: if Y is continuous with pdf $f_Y(y)$:   \hfill \\
Var($Y$) = $\sigma ^2 = E[(Y - \mu)^2] = \int_{-\infty}^{\infty}(y- \mu)^2 \cdot f_Y(y) dy$   \hfill \\  
\hfill \\  
Discrete or continuous: $Var(W) = E(W^2) - (E(W))^2 = E(W^2) - \mu ^2$\hfill \\  
\hfill \\  

Variances always add (independence not required).  And, 
$Var(aX + bY) = a^2\cdot Var(X) + b^2\cdot Var(Y) $ {\tiny (page 189)} \hfill \\  
\hfill \\  

\textbf{Standard Deviation}: $\sigma$.  Square root of variance. \hfill \\  

\subsection{Misc.}
$F_X(x) = P(X \leq x)$  \hfill \\   
$p_X(x) = P(X=x) = P(X \leq x) - P(X < x)$ (discrete)  {\tiny (lec 10/28)}  \hfill \\  
 \hfill \\  
 PDFs can have values greater than 1. 

\section{Multiple random variables} 
\textbf{Double sigma notation}: If you have two $\sum$s, you have a 2D grid of values.  (\href{<https://www.youtube.com/watch?v=BK7OEt9AHIw>}{simple video}).  If the two dimensions start and stop at numbers (or maybe $\infty$) then you have a value in every position of the grid.  There can be cases, however, where the values of the two variables are interdependent (\href{<http://math.stackexchange.com/questions/490723/double-summation-switch>}{example}). 

Example with coefficients that depend on both indices (hasn't come up yet): 
Consider real numbers $a_{ij}$, where $i$ ranges from 1 to 3, and $j$, from 1 to 2. We thus have the following equality:
$$\sum_{i=1}^3 \sum_{j=1}^2 a_{ij}=\sum_{i=1}^3(a_{i1}+a_{i2})=(a_{11}+a_{12})+(a_{21}+a_{22})+(a_{31}+a_{32}).$$


\textbf{Vocab}:  \hfill \\
\textbf{joint density} - pdf for a joint distribution. \hfill \\
\textbf{univariate distribution} - pdf of distribution with exactly 1 random variable \hfill \\
\textbf{marginal distribution} - \hfill \\
\hfill \\
If you have a joint distribution with 2 random variables, the two marginal distributions are univariate distributions \hfill \\
\hfill \\

See Elizabeth's single-page PDF for lots of formulas. 
Key ones:  {\tiny 10/28 lecture}

\textbf{One Dimension:}  $ P(a \leq b) = P(X \leq b) - P(X \leq a) = F_X(b) - F_X(a)$   \hfill \\
\textbf{Two Dimensions:} $ P(a_1 < x \leq a_2 , b_1 < y \leq b_2) = F_{X,Y}(a_2, b_2) - F_{X,Y}(a_1, b_2) - F_{X,Y}(a_2, b_1) + F_{X,Y}(a_1, b_1) $

\textbf{Marginal CDFs:} If you have $F_{X,Y}(a,b)$, \hfill \\ 
$F_X(a) = P(X \leq a, y < \infty)$ = $F_{X,Y}(a, \infty)$   \hfill \\ 

See Elizabeth's PDFs for a lot more formulae. 

%\begin{center}
%\begin{tabular}{ c c c }
% \textbf{R.Var.} & \textbf{1 variable/dimension} & \textbf{2 variables/dimensions}  \\ 
 %joint CDF  & $  \begin{tabular}{@{}c@{}} $ P(a \leq b) = P(X \leq b) - P(X \leq a) $  \\  $= F_X(b) - F_X(a) $ \end{tabular} $ &  
%  			\begin{tabular}{@{}c@{}} $ F_{X,Y}(a , b) = P(X \leq a \cap y \leq b) $  \\  $= F_X(b) - F_X(a) $ \end{tabular}  \\  
%\end{tabular}
%\end{center}
%\hfill \\


\subsection{Expected values for functions of random variables}
\textbf{NOTE: Expectations by default don't have X, Y in them.}  

{\tiny (pg 150; Lecture 10/14)}  Suppose $X$ is a discrete random variable with pdf $p_X(k)$.  Let $g(X)$ be a function of $X$.  Then the expected value of the random variable $g(X)$ is given by: \hfill \\
	\[ E[g(X)] = \sum\limits_{\mbox{all k}} g(k) \cdot p_X(k) \]
Provided that $\sum\limits_{\mbox{all k}} | g(k)| \cdot p_X(k) < \inf$
\hfill \\
\hfill \\
If $Y$ is a discrete random variable with pdf $f_Y(y)$, and if $g(Y)$ is a continuous function, then the expected value of the random variable $g(Y)$ is: \hfill \\
	\[ E[g(Y)] = \int\limits_{- \inf}^{\inf} g(y) \cdot f_Y(y)dy \]
Provided that $E[g(Y)] = \int\limits_{\inf}^{\inf} | g(y)| \cdot f_Y(y)dy < \inf$  \hfill \\
\hfill \\

These two definitions highlight the difference in nomenclature.  See page 127 for definition of discrete cdf.  See page 135 for definition of continuous cdf 
\begin{center}
\begin{tabular}{ c c c c c }
 \textbf{R.Var.} & \textbf{x-axis} & \textbf{pdf notation} & \textbf{cdf notation}   \\ 
 $\mbox{\tiny discrete  } X$     & $k$ & $p_X(k)  \mbox{ \tiny or }  P(X=k) $ & $F_X(k) \mbox{ \tiny or } P(X \leq k)  $  \\  
$\mbox{\tiny continuous  } Y$ & $y$ & $f_Y(y)$                                          & $F_Y(y) \mbox{ \tiny or } P(Y \leq y)  $ 
\end{tabular}
\end{center}
\hfill \\


 \begin{center}
\begin{tabular}{ c c c c c }
 \textbf{R.Var.} & \textbf{pdf} & \textbf{cdf}  \\ 
 {discrete  } $X$    & \begin{tabular}{@{}c@{}} $ p_X(x) = P(X=x)$   \\  $= P(X \leq x) - P(X <x)$ \end{tabular}         & $F_X(x)  =  \sum\limits_{z \leq x}p_X(z)$  \\  
 {continuous  } $Y$ & $p_X(x) = 0$ {\tiny (at one point)}  & $F_Y(y) = \int_{-\infty}^{y}f_Y(y)$ \\  
\end{tabular}
\end{center}
\hfill \\
 
\textbf{NOTE:}  Need to have intuitive understnading that $P$s are probabilities and $F$s are cdfs.  Can't just assume capitol letters are cdfs.
%When we start talking about transforms of random variables, you can get $P$ that isn't CDF.  
E.g. if $Y = X^3$ then the $P$s in $P(Y = y) = P(X^3 = y) = P(X = y^{1/3}) $ are probability, not cdf.  Note that there is no subscript in this case.  
%But that it looks rather like $P(X=k)$ in table above.   % $P(X) \mbox{ \tiny or } P(X = y)$

\hfill \\
Say your random variable is the number on a die face.  Say your function of that random variable is the square of that number.  If you want the expectation of the square, you can sum the products of the squares with the probabilities of the random variable values.  \hfill \\
\hfill \\
\textbf{Theorem} {\tiny (10/14 Lecture \& similar on pg 150)}:  \hfill \\
let $Y = g(X)$.  Since:
	\begin{align*}
		\mathbb{E} (Y) &=  \sum\limits_{y} yP(Y=y)   \\
			& = \sum\limits_{y} y \sum\limits_{x:g(x)=y} P(X = {\cal X})  \\
			& \mbox{note: above is a sum over the $y$ values,} \\ 
			& \mbox{and a sum over the $x$s that can give those y values.} \\
			& \mbox{There can be multiple $x$s that give a particular $y$.} \\
			& = \sum\limits_{y} \sum\limits_{x:g(x)=y} g(x)P(X=x) \\
			& \mbox{Drop $y$ sum b/c now everything is in terms of $x$} \\
			& \mbox{You are summing over all the events in a set that }   \\
			& \mbox{are indexed by y.  But when you take the sum over y, }  \\
			& \mbox{you say let's sum over all the different outcomes. } \\
			& = \sum\limits_{x \in {\cal X}} g(x)P(X=x) = \sum\limits_{x \in {\cal X}} g(x)P({\cal X}) \\ 
			& \mbox{This is a sum over all the $x$s in the whole set of outcomes (${\cal X}$)} 
	\end{align*} 
Note that this works for $g(Y) = Y^2$.  $E(Y^2) =  \int_{-\inf}^{\inf} Y^2 \cdot f_Y(y)dy  $.  {\tiny 10/21 review lecture}  \hfill \\
Loop over all the values the random variable takes (all $x$s in ${\cal X}$), and sum (probability of that $x$)*(the function applied to that $x$).  \hfill \\
 \hfill \\
 
\textbf{Addition of functions of random variables} 
Adding sums of expectations always works; doesn't require independence. { \tiny (10/14 lecture)}
	\begin{align*}
		\mathbb{E} (g_1(x) + g_2(x)) &=   \sum\limits_{x \in {\cal X}} (g_1(x) + g_2(x))p(x) \\
			&= \sum\limits_{x \in {\cal X}} g_1(x)p(x) + \sum\limits_{x \in {\cal X}} g_2(x)p(x) \\
			&=\mathbb{E}  (g_1(x)) + \mathbb{E} (g_2(x)) 
	\end{align*} 
Simplest case: $E(X + Y) = E(X) + E(Y)$, valid even if X is not statistically independent of Y. \hfill \\
 \hfill \\
 
 
Example $\mathbb{E}  (a(x) + b)$:
	\begin{align*}
		\mathbb{E} (a(x) + b) &=  \mathbb{E}  (a(x)) + \mathbb{E} (b) \\
			& = a\mathbb{E} (x) + b
	\end{align*} 
Example $\mathbb{E}  (x(x-1))$:
	\begin{align*}
		\mathbb{E} (x(x-1)) &= \mathbb{E} (x^1 - x)) \\
			& =  \mathbb{E}  (x^2) - \mathbb{E} (x)    \\
			& \mbox{this might be easier to evaluate}
	\end{align*} 
Example $\mathbb{E}  (X^2 + Y^2)$:  {\tiny HW 5, Q 3.9.10}
	\begin{align*}
		\mathbb{E} (X^2 + Y^2) &= \mathbb{E}(X^2) + \mathbb{E}(Y^2) \\
	\end{align*} 
	
\section{Examples}
\textbf{Different ways shapes can come up:} \hfill \\
\underline{Probability of landing below some x value within a weird 2D shape.}  {\tiny 10/19 lecture}  \hfill \\
 If you have a diamond with points on the X and Y axes, but the top/bottom is cur off, what is probability of landing at some x?
 (What is $f_X(x)$?)  
 You can't just derive something because X is less probable as you go out to the points.  
 Use the classic trick: start with the CDF. v
 $F_X(x) = P(X \leq x) = 1 - F_X(x) = P(X \geq x) $
 \hfill \\
 \hfill \\
\underline{Probability of a subset of events.}  \hfill \\
* E.g. $Y > 2X$.  If you have a $f_{X,Y}(x,y)$ and you want to know what the probability is that $Y > 2X$ you can draw the shape (which should be smaller than the normal ranges of X and Y) and integrate both variables. \hfill \\
Integrate one using its two obvious limits and the other one using the relationship to the other variable.  {\tiny (E.g. Example 3.7.4 pg 165)}.  \hfill \\
Or if the distribution is uniform for the range of X, Y your $P(X,Y \mbox{ in some subset of X and Y values}) = $ fraction of the area.  \hfill \\
   \hfill \\

\underline{A shape imposing restrictions on allowable values of X or Y.}   (Indicator function multiplied on) \hfill \\
* E.g. $f_{X,Y}(x,y)=1/x, \mbox{ for } 0 \leq y \leq x \leq 1$ {\tiny (problem 3.7.20b)}.  If you want $f_X(x)$ or $f_Y(y)$ you have to use the $y <x$ relationship in the integral bounds.   Be careful with the integral bounds when getting cdf $F$s and expectations, too!  \hfill \\
* Or, problem 3.7.22 has $f_{X,Y}(X,Y) = 2e^{-x}e^{-y}$ but it is only valid for $y > x$ and 0 otherwise.  
Here the indicator function is ruining the independence.  I.e. the allowable shape is affecting the sample space.  
This will affect the CDF (e.g. integrate y from 0 to x), and the expectation.  It will also affect the marginal probabilities for either X or Y.   Use the $y < x$ bound (or $x > y$ bound) in each integration.  \hfill \\
\hfill \\ 

 \underline{A shape that represents a function of one or more random variables. }  \hfill \\
** E.g. area of triangle formed by (X,0), (0,Y), (0,0) for X, Y uniform on [0,1].
In this case the shape is only representing an aggregation of the variables.  It does *not* represent restricting values of either (e.g. $X < Y$).   
Then $W = (1/2)XY$.  X,Y uniform $ \rightarrow$ $E(XY/2) = (1/2)\cdot \mathbb{E}(X)\mathbb{E}(Y)$.  What is $f_{X,Y}(x,y)$ in this case?  Independence allows us to know $ f_{X,Y}(x,y)  \propto f_X(x)f_Y(y)$ since the limits don't affect each other (both valid over [0,1]).  \hfill \\
\hfill \\
     
 A shape that represents a function of one or more random variables.   \hfill \\
 \textbf{NO.  this isn't a shape.  It is a line.}
* $Y = X^2$ Here we could draw a line $Y = X^2$.  If we assume X is uniformly distributed, $f_Y(y) = c \cdot y^2$.   \hfill \\
** Now say X is not uniform.  Instead $f_X(x)=3x^2$ for $0 < x < 1$ then $F_Y(y) = P(Y \leq y) = P(X^2 \leq y) = P(X \leq y^{1/2}) = F_X(y^{1/2})$.  
We don't have $F_X(x)$ but can get it; it is $x^3$.  
Then $F_Y(y) = F_X(y^{1/2}) = (y^{1/2})^3 = y^{2/3}$ and $f_Y(y) = \frac{2}{3} y^{1/3}$  
We can get $\mathbb{E}(Y)$ two ways now.  \hfill \\
(Way 1:) This is always true: $E(Y)=E(X^2)$.  
We can use independence so $E(Y)=E(X^2)=(E(X))^2$.  
And $E(X)=\int_0^1 x \cdot f_X(x) = \int_0^1 x \cdot 3x^2 = 3/4 $.  Square that to get $9/16$.     \hfill \\
(Way 2:) Or, $E(g(x)) = \int_0^1 g(x) \cdot f_X(x)  = \int_0^1 x^2 \cdot 3x^2 dx$      \hfill \\
\hfill \\


-- ?? A shape restricting values of one or more variable(s).  Say    \hfill \\
	
\section{Joint Density}

$F_{X,Y}(a, b) = P(x \leq a \cap y \leq b) $.  \hfill \\
The $ \cap $ is often implicit: $P(x \leq a, y \leq b)$  {\tiny (10/28 lecture)}
 \hfill \\
 \hfill \\
 To get marginal: $F_X(a) = P(x \leq a) = P(X \leq a, y \leq \inf)$  {\tiny (10/28 lecture)}  \hfill \\
$ = F_{X,Y}(a, \inf)$   \hfill \\
  \hfill \\

If you are given $f_X(x)$ and $f_Y(y)$ you can't know the joint distribution *unless* you are told they are independent.  
 There are many univariate distributions that can lead to a particular joint distribution if the two variables are not independent.  {\tiny (11/3 TA lecture)}  \hfill \\
  \hfill \\


\textbf{Independence of random variables}.  {\tiny (page 175)} \hfill \\
Basic: Events $E_1$ and $E_2$ are independent iff $P(E_1 \cap E_2) = P(E_1) \cdot P(E_2)$  \hfill \\
 \hfill \\
For Random Variables: $E_1 \equiv \{ X \in A \}$, $E_2 \equiv \{ Y \in B \}$.  
Then if for \textit{all} subsets of $A$ and $B$, $P(X \in A, Y \in B) =P(X \in A) \cdot P(Y \in B) $ 
But if you had to check for every single A nd B it wouldn't be very useful.  
So, \textbf{Discrete Case}: {\tiny 10/30 lecture} \hfill \\  
\begin{align*}
	\mbox{Take } & A=\{X\}, B=\{Y\}  \\
	 \mbox{$X$, $Y$ independent: } & P(X=x, Y=y) = P(X=x) \cdot P(Y=y)   \\
	P(X \in A, Y \in B) &=  \sum\limits_{x \in {\cal A}} \sum\limits_{y \in {\cal B}} p_{X,Y}(x,y) \\
	&=  \sum\limits_{x \in {\cal A}} \sum\limits_{y \in {\cal B}} p_{X}(x)  \cdot p_{Y}(y) \\
	&= ( \sum\limits_{x \in {\cal A}} p_{X}(x)) ( \sum\limits_{y \in {\cal B}}   \cdot p_{X}(y) )\\
	&= P(X \in A) \cdot P(Y \in B)
\end{align*}
\hfill \\

\textbf{10/30 Independence facts} (10/30 lecture)  \hfill \\
Probabilities don't depend on each other: $P(X \leq x, Y \leq y) = P(X \leq x) \cdot P(Y \leq y)$ \hfill \\
CDFs don't depend on each other {\tiny (pg 174)}: $F(X \leq x, Y \leq y) = F(X \leq x) \cdot F(Y \leq y)$ \hfill \\
Also: $f_{X,Y}(x,y) \propto g_1(x)g_2(y)$ \textbf{BUT} limits that tie X and Y can make this break.  Recall the 10/30 example:  $0 < y < x < 1$ for $f_{X,Y}(x,y)=cxy$.  $cxy$ looks separable; be careful!

\hfill \\
\textbf{notation example:}
$f_{X_1, X_2 , \dots, X_n}(x_1, x_2 , \dots, x_n) = g_1(x_1) g_2(x_2) \dots g_n(x_n)$. \hfill \\
\hfill \\
Example: if $X_1$, $X_2$, and $X_3$ are independent random variables each with pdf $f_{X_i}(x_i) = 4x_i^3$. 
Then $f_{X_1, X_2, X_3}(x_1, x_2, x_3) = (4x_1^3)(4x_2^3)(4x_3^3) = 4^3 x_1^3 x_2^3 4x_3^3$ \hfill \\
\hfill \\

Example: {\tiny (10/30 lecture)} \hfill \\
$f_{X,Y}=c \cdot x \cdot y$ for $0 < x < 1$ and $0 < y < 1$.   
These are independent because you can pull the joint probability apart into single probabilities. \hfill \\
But if you have $f_{X,Y}=c \cdot x \cdot y$ for $0 < y < x  < 1$ they are not independent.  
You can tell right away because the bounds are dependent on each other.  You can also draw the $y=x$ line and shade the  $0 < y < x  < 1$ region to see. 
\hfill \\
\hfill \\

\textbf{memoryless property}.  You are waiting for a phone call.  The probability of the phone ringing in the next minute is constant and doesn't depend on how long you have been waiting.  More formally: X = phone call time (?)   \hfill \\
$P(X \geq s + t \mid x \geq s) = P(x \geq t) $ \hfill \\
$ $  \hfill \\

\textbf{Combining random variables}
If they are independent, you can multiply the independent distributions. 


Example from 10/26 TA review lecture: 
Say you have 50 random variables that are unform on $[0, 1]$.  $f_X1 = f_X2 = ... f_X50$.
The cdf is $F_X(x) = \{0 \mbox{ if } x < 0, x \mbox{ if } x \in [0, 1], 1 \mbox{ if } x > 1\}$.
Define $Y \equiv \mbox{max}(X_i)$.  Since they are independent, we know 
\begin{align*}
	P(Y \leq y) &= P(X_1 \leq y \mbox{ and } X_2 \leq y \mbox{ and }\dots \mbox{ and } X_{50} \leq y) \\
			& \mbox{\tiny independence allows multipication} \\
			&= P(X_1 \leq y) \cdot P(X_2 \leq y) \cdot \dots \cdot P(X_{50} \leq y) \\
			&= F_{X_1}(y) \dots F_{X_{50}}(y) =  y^{50} \\
		P(Y \leq y) &= y^{50} 
\end{align*}
{\tiny differentiate to convert from } $F_Y$ to $f_Y$:  $f_Y(y)=50y^{49}$  \hfill \\
\hfill \\

Example: Getting the expectation of one variable from a joint distribution {\tiny (10/30 lecture)}  \hfill \\
If you have $f_{X,Y}(x,y)$, and want $E(X)$, you can get $f_X(x) = \int\limits_{y} f_{X,Y}(x,y)dy$.   \hfill \\
Then You can get $E(X) = \int_{-\infty}^{\infty} x \cdot f_X(x)dx$.    \hfill \\
You can also get $F_{X,Y} = \int_{-\infty}^{x} \int_{-\infty}^{y} f_{X,Y}(u,v) du dv$ \hfill \\
Then you can get $F_X(x) = F_{X,Y}(x, y = \infty)$ \hfill \\

			
\section{Transforming/Combining Random Variables}

{\tiny (11/2 TA lecture)}  X and Y are independent: $f_X(x)= \frac{1}{2 \pi}exp(-x^2/2)$, $f_Y(y)= \frac{1}{2 \pi}exp(-y^2/2)$.   \hfill \\
Question: If $U = X^2 + Y^2$, what is $F_U(u)$?  \hfill \\
Answer: $f_{X,Y}(x,y)=f_X(x)f_Y(y) = \frac{1}{2 \pi} exp((-1/2)(x^2 + y^2))$ \hfill \\

\subsection{Location/Scale}
\textbf{$Y = X + b$:} 
\begin{align*}
	F_Y(y) &= P(Y \leq y) \\
			&= P(X \leq y-b)  \\
			&= F_X(y - b)  \\
			& \mbox{differentiate} \\
	      \mathbf{f_Y(y)} & \mathbf{= f_X(y-b)}
\end{align*}
?? Do you see why we are subtracting $c$, not adding it?  $f_{x+c}(t) = f_X(t-c) = P(X=t-c) = P(X+c=t)$  {\tiny (11/2 TA lecture)}

\subsection{Scaling Variables}
\textbf{$Y = aX $}: and $a > 0$. 
\begin{align*}
	F_Y(y) &= P(Y \leq y) \\
			&= P(aX \leq y)  \\
			&= P(X \leq y/a)  \\
			&= F_X(y/a)  \\
			& \mbox{differentiate} \\
		\mathbf{f_Y(y)} & \mathbf{= (1/a) \cdot f_X(y/a)}
\end{align*}

\subsection{Location and Scale}
\textbf{$Y = aX + b $}: (and $a > 0$). 
\begin{align*}
	F_Y(y) &= P(Y \leq y) \\
			&= P(aX + b \leq y)  \\
			&= P(X \leq (y-b)/a)  \\
			&= F_X((y-b)/a)  \\
			& \mbox{differentiate} \\
		\mathbf{f_Y(y)} & \mathbf{= (1/a) \cdot f_X((y-b)/a)}
\end{align*}
Visualize the uniform distribution over $[0,1]$.  When you have $W = aX+B$, you scale down so the height is now $1/a$ over $[0, a]$.  Then you shift it to $1/a$ over $[b, b+a]$.   {\tiny (10/30 lecture; last page)} \hfill \\
  \hfill \\
Example: (10/30 lecture) (??) Linear function of a normal distribution is a normal distribution.  The mean and variance drop out. (??)   \hfill \\

\subsection{Tricks for adding and multiplying random variables}

\textbf{you can get mixures of fs, Ps, and Fs and still make the connection.}
Start with converting things to Fs, then use Ps, then move back to fs if desired.  See dog eared notebook page of MT2 practice problems.  And Lecture 10/30.  {\tiny This works great for location/scale.  I'm not sure how applicable it is for R.V. addition. }\hfill \\
\hfill \\

\textbf{example: add two variables}.  X and Y are uniform on $[0,1]$.  Find pdf of $Z = X + Y$.  Note that your range for Z is [0, 2].  Expect lots of probability around $Z = 1$, and zero at $Z = 0$ and $Z = 1$ 
\begin{align*}
	F_Z(z) &= P(Z \leq z) = P(X + Y \leq z) \\
	& \mbox{\tiny use conditional probability \& independence of X \& Y} \\
	&= \int_{- \infty}^{\infty} P(X + Y \leq z \mid X = x) \cdot f_X(x)dx
\end{align*}

\section{Covariance}
{\tiny (11/4Lecture)}
Covariance is a measure of how much two random variables change together. \hfill \\
The sign of the covariance therefore shows the tendency in the linear relationship between the variables.   \hfill \\
The magnitude of the covariance is not easy to interpret.  \hfill \\
Variance is a special case of the covariance when the two variables are identical.   \hfill \\
The normalized version of the covariance, the correlation coefficient, however, shows by its magnitude the strength of the linear relation.  \hfill \\
  \hfill \\
 Recall that Var (variance; $\sigma ^2$) is $E[(X - \mu)^2]$.  And that $E[(X - \mu)] = 0$.  {\tiny (pg 156)} \hfill \\
 
 \hfill \\
$\mbox{Cov}(X,Y)=\mathbb{E}[(X-\mathbb{E}X)(Y- \mathbb{E}Y)]$  \hfill \\
If you multiply it out: $\mbox{Cov}(X,Y)=(\mathbb{E}(XY)) - (\mathbb{E}X)(\mathbb{E}Y)$  {\tiny (pg 189)}  \hfill \\
 \hfill \\
 If $X$ and $Y$ are independent, Cov$(X,Y)=0$.  Of course you can have $Cov(X,Y)=0$ without independence {\tiny (page 189)}.  \hfill \\
  \hfill \\
 Var$(X + Y) = \mathbb{E}((X+Y)^2) -  \mathbb{E}((X+Y))^2$
 $= Var(X) + 2 Cov(X,Y) + Var (Y)$. 
 If $X$, $Y$ are independent, $Var(X+Y) = Var(X) + Var (Y)$. \hfill \\

\hfill \\

\textbf{Covariance versus Variance}: 
A covariance refers to the measure of how two random variables will change together and is used to calculate the correlation between variables. 
The variance refers to the spread of the data set Ñ how far apart the numbers are in relation to the mean, for instance. \hfill \\
\hfill \\

\textbf{$\mbox{Cov}(X,X) = \mbox{Var}(X)$}. {\tiny Midterm 2 practice problem.} \hfill \\
Because $\mbox{Cov}(X,X)=(\mathbb{E}(XX)) - (\mathbb{E}X)(\mathbb{E}X)$.  We can solve for $\mathbb{E}(XX)$ using the fact that correlation = 1 and $\rho(X,X)=\mbox{cov}(X,Y)/\sqrt{\mbox{var}(X)\mbox{var}(X)} = \mbox{cov}(X,X)/\mbox{var}(X)$.
 $1 = \rho(X,X)= \mbox{cov}(X,X)/\mbox{var}(X)$ so \textbf{$ \mbox{cov}(X,X) = \mbox{var}(X)$}.

\section{Correlation}

{\tiny (Lec. 11/4)}  Correlation: $\rho = \frac{\mbox{Cov}(X,Y)}{\sqrt{\mbox{Cov}(X)\mbox{Cov}(Y)}}$
Bounded by $-1 \leq \rho \leq 1$ \hfill \\
When is it 1 or -1?  When Var$(X-bY) = 0$.  Var $= 0$ when the probability mass is all at one point, not spread out. \hfill \\
This happens when X is a linear function of Y. \hfill \\

\section{Expectation, Variance, Covariance, and Correlation}
\textbf{Scaling \& shifting: }  \hfill \\
$E(aX + b) = aE(X) + b$  \hfill \\
$Var(aX + b) = a^2 Var(X)$  {\tiny(b doesn't matter.  Squared is b/c Var = $\sigma^2$; $\sigma$ scales w/ a so you get $a^2$)} \hfill \\
$Cov(aX + b, Y) = a \cdot Cov(X,Y)$ {\tiny(Has units, so keep factor of a.  b doesn't matter)}  
		Also: $Cov(aX + b, cY + d) = a\cdot c \cdot Cov(X,Y)$ \hfill \\
$Corr(aX + b, Y) = Corr(X, Y)$  {\tiny Use properties above to see.  Also, dimensionless so a can't be there.} \hfill \\
\hfill \\

\textbf{Cov, and Corr with itself: }  {\tiny Note that you can't have E or Var of something w/ itself: univariate. }  \hfill \\
$Cov(X, X) = Var(X)$  {\tiny From the definition of Variance. } \hfill \\
$Corr(X, X) = 1$ \hfill \\
 \hfill \\
 

\textbf{Cov, and Corr with an independent var.}  Say X, Y are independent. \hfill \\
$E(XY) = E(X)E(Y)$    {\tiny Can get there from a g(X,Y) idea.  g(X,Y) is xy.  
	Then $E(XY) = \int \int g(X,y) \cdot f_{X,Y}(x,y) =  \int \int xy \cdot f_{X,Y}(x,y) =  \int x \cdot f_X(x) \int y \cdot f_{Y}(y) $ . }   \hfill \\
$Cov(X, Y) = 0$   and  $Cov(aX, bY) = 0$  \hfill \\ \hfill \\
$Corr(X, Y) = 0$   \hfill \\
\hfill \\

\textbf{With constants:}  \hfill \\
$E(a) = aE(1) = a$  \hfill \\
$Var(a) = a^2 Var(1) = 0$  {\tiny ($=E[(a-E(a))^2] = E[(a-a)^2] = E[0] = 0$)} \hfill \\
$Cov(a, Y) = a \cdot Cov(1,Y)= a \cdot 0 = 0$ {\tiny(E((1 - E(1))) = 0.  Also Y isn't dependent on a.}  \hfill \\
$Corr(a, Y) =  0 $ {\tiny But it doesn't really make sense to talk about corr w/ constant.  Lec 11/4 }  \hfill \\
\hfill \\



